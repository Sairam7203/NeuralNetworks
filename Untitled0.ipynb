{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIMQwVoIDmxPpDmM02zehE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sairam9010/NeuralNetworks/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U85ufz3_hSwz",
        "outputId": "ff55f536-3582-41d2-b7b1-851f94c5ece1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Original Tokens:\n",
            "['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri', '.']\n",
            "\n",
            "2. Tokens Without Stopwords:\n",
            "['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri', '.']\n",
            "\n",
            "3. Stemmed Words:\n",
            "['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK\n",
        "!pip install nltk --quiet\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download only what's necessary\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_nlp(sentence):\n",
        "    # 1. Tokenization using TreebankWordTokenizer (avoids punkt issue)\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    print(\"1. Original Tokens:\")\n",
        "    print(tokens)\n",
        "\n",
        "    # 2. Remove Stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_no_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
        "    print(\"\\n2. Tokens Without Stopwords:\")\n",
        "    print(tokens_no_stopwords)\n",
        "\n",
        "    # 3. Apply Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens_no_stopwords]\n",
        "    print(\"\\n3. Stemmed Words:\")\n",
        "    print(stemmed_words)\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "preprocess_nlp(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spaCy and download the English model\n",
        "!pip install spacy --quiet\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Import spaCy\n",
        "import spacy\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
        "\n",
        "# Process the sentence\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract and print named entities\n",
        "print(\"Named Entities:\\n\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Text: {ent.text}\")\n",
        "    print(f\"Label: {ent.label_}\")\n",
        "    print(f\"Start Char: {ent.start_char}, End Char: {ent.end_char}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N9YJEuSh3Ed",
        "outputId": "e553454a-8b95-4a17-db12-45f42409ef17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Named Entities:\n",
            "\n",
            "Text: Barack Obama\n",
            "Label: PERSON\n",
            "Start Char: 0, End Char: 12\n",
            "----------------------------------------\n",
            "Text: 44th\n",
            "Label: ORDINAL\n",
            "Start Char: 27, End Char: 31\n",
            "----------------------------------------\n",
            "Text: the United States\n",
            "Label: GPE\n",
            "Start Char: 45, End Char: 62\n",
            "----------------------------------------\n",
            "Text: the Nobel Peace Prize\n",
            "Label: WORK_OF_ART\n",
            "Start Char: 71, End Char: 92\n",
            "----------------------------------------\n",
            "Text: 2009\n",
            "Label: DATE\n",
            "Start Char: 96, End Char: 100\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    # 1. Dot product of Q and Kᵀ\n",
        "    scores = np.dot(Q, K.T)\n",
        "\n",
        "    # 2. Scale by sqrt(d), where d is key dimension\n",
        "    d = K.shape[-1]\n",
        "    scaled_scores = scores / np.sqrt(d)\n",
        "\n",
        "    # 3. Apply softmax to get attention weights\n",
        "    def softmax(x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))  # for numerical stability\n",
        "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "    attention_weights = softmax(scaled_scores)\n",
        "\n",
        "    # 4. Multiply attention weights by V\n",
        "    output = np.dot(attention_weights, V)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Attention Weights:\\n\", attention_weights)\n",
        "    print(\"\\nFinal Output:\\n\", output)\n",
        "\n",
        "# Test inputs\n",
        "Q = np.array([[1, 0, 1, 0],\n",
        "              [0, 1, 0, 1]])\n",
        "K = np.array([[1, 0, 1, 0],\n",
        "              [0, 1, 0, 1]])\n",
        "V = np.array([[1, 2, 3, 4],\n",
        "              [5, 6, 7, 8]])\n",
        "\n",
        "# Run the attention function\n",
        "scaled_dot_product_attention(Q, K, V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sREpPnV7ink0",
        "outputId": "5a16f28e-5fca-4521-cca6-088486986a18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.73105858 0.26894142]\n",
            " [0.26894142 0.73105858]]\n",
            "\n",
            "Final Output:\n",
            " [[2.07576569 3.07576569 4.07576569 5.07576569]\n",
            " [3.92423431 4.92423431 5.92423431 6.92423431]]\n"
          ]
        }
      ]
    }
  ]
}